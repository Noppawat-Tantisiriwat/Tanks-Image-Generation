{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of this notebook will be dedicated to the basic VAE architecture\n",
    "\n",
    "# The second part will be dedicated to the VAE with skip-connections\n",
    "\n",
    "# both model will be tested by passing [64, 3, 64, 64] torch.randn tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                conv_filters_in: int,\n",
    "                conv_filters_out: int,\n",
    "                conv_kernels: Tuple[int],\n",
    "                conv_strides: Tuple[int],\n",
    "                paddings: Tuple[int],\n",
    "                dilations: Tuple[int],\n",
    "                **kwargs):\n",
    "        \n",
    "        super(Conv2dBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_filters_in = conv_filters_in\n",
    "        self.conv_filters_out = conv_filters_out\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.dilations = dilations\n",
    "\n",
    "\n",
    "        self.conv2d = nn.Conv2d(in_channels=self.conv_filters_in, \n",
    "                                out_channels=self.conv_filters_out, \n",
    "                                kernel_size=self.conv_kernels,\n",
    "                                stride=self.conv_strides,\n",
    "                                padding=self.paddings,\n",
    "                                dilation=self.dilations)\n",
    "\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=self.conv_kernels, \n",
    "                                    stride=self.conv_strides,\n",
    "                                    padding=self.paddings,\n",
    "                                    dilation=self.dilations)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=self.conv_filters_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        x = self.maxpool2d(x)\n",
    "\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTranspose2dBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                conv_filters_in: int,\n",
    "                conv_filters_out: int,\n",
    "                conv_kernels: Tuple[int],\n",
    "                conv_strides: Tuple[int],\n",
    "                paddings: Tuple[int],\n",
    "                output_paddings: Tuple[int],\n",
    "                dilations: Tuple[int],\n",
    "                **kwargs):\n",
    "\n",
    "        super(ConvTranspose2dBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_filters_in = conv_filters_in\n",
    "        self.conv_filters_out = conv_filters_out\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.output_paddings = output_paddings\n",
    "        self.dilations = dilations\n",
    "\n",
    "        self.convtranspose2d = nn.ConvTranspose2d(in_channels=self.conv_filters_in, \n",
    "                                                out_channels=self.conv_filters_out, \n",
    "                                                kernel_size=self.conv_kernels,\n",
    "                                                stride=self.conv_strides,\n",
    "                                                padding=self.paddings,\n",
    "                                                output_padding=self.output_paddings,\n",
    "                                                dilation=self.dilations)\n",
    "\n",
    "        self.upsampling = nn.UpsamplingBilinear2d(scale_factor=self.conv_strides[0])\n",
    "\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=self.conv_filters_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.convtranspose2d(x)\n",
    "\n",
    "        x = self.upsampling(x)\n",
    "\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_shape: List[int],\n",
    "                conv_filters:List[int], # must have 1 more element than others -> FIRST element must be 3 for colored images\n",
    "                conv_kernels: List[Tuple[int]],\n",
    "                conv_strides: List[Tuple[int]],\n",
    "                paddings: List[Tuple[int]],\n",
    "                dilations: List[Tuple[int]],\n",
    "                latent_space_dim: int,\n",
    "                **kwargs):\n",
    "    \n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        \n",
    "        self.conv_filters = conv_filters # [2, 4, 8]\n",
    "        self.conv_kernels = conv_kernels \n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.dilations = dilations\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "\n",
    "        \n",
    "        # dim assertion\n",
    "\n",
    "        assert len(self.conv_kernels) == len(self.conv_strides) == len(self.paddings)\n",
    "\n",
    "        self.convblocks = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "            (\n",
    "            f\"Convolution_Block_{i+1}\",\n",
    "            Conv2dBlock(conv_filters_in=self.conv_filters[i],\n",
    "                        conv_filters_out=self.conv_filters[i+1],\n",
    "                        conv_kernels=self.conv_kernels[i],\n",
    "                        conv_strides=self.conv_strides[i],\n",
    "                        paddings=self.paddings[i],\n",
    "                        dilations=self.dilations[i]).float()\n",
    "            )\n",
    "            \n",
    "            for i in range(len(self.conv_filters) - 1)\n",
    "            \n",
    "                ]\n",
    "            )\n",
    "        )   \n",
    "\n",
    "        self.shape_before_bottleneck = self._calculate_shape_before_bottleneck(input_shape).shape\n",
    "\n",
    "        self.flatten_shape = torch.numel(self._calculate_shape_before_bottleneck(input_shape))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.mu = nn.Linear(self.flatten_shape, self.latent_space_dim)\n",
    "\n",
    "        self.log_sigma = nn.Linear(self.flatten_shape, self.latent_space_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def _calculate_shape_before_bottleneck(self, input_shape: List[int]):\n",
    "\n",
    "        x = torch.ones(input_shape)\n",
    "\n",
    "        x = torch.unsqueeze(x, 0) \n",
    "\n",
    "        for convblock in self.convblocks:\n",
    "\n",
    "            x = convblock(x)\n",
    "\n",
    "        return x   \n",
    "\n",
    "    def _reparameterized(self, mu, log_sigma):\n",
    "        \n",
    "        eps = torch.randn(size=mu.shape)\n",
    "        \n",
    "        sample_point = mu + torch.exp(log_sigma / 2) * eps\n",
    "        \n",
    "        return sample_point    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for convblock in self.convblocks:\n",
    "            \n",
    "            x = convblock(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_sigma= self.log_sigma(x)\n",
    "\n",
    "        x = self._reparameterized(mu, log_sigma)\n",
    "\n",
    "        return x, (mu, log_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                latent_space_dim :int,\n",
    "                shape_before_bottleneck : torch.Size,\n",
    "                conv_filters : List[int], # must have 1 more element than others -> LAST element must be 3 for colored images\n",
    "                conv_kernels : List[Tuple[int]],\n",
    "                conv_strides : List[Tuple[int]],\n",
    "                paddings : List[Tuple[int]],\n",
    "                output_paddings : List[Tuple[int]],\n",
    "                dilations : List[Tuple[int]],\n",
    "                out_channel : int,\n",
    "                **kwargs):\n",
    "\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_filters = conv_filters\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.output_paddings = output_paddings\n",
    "        self.dilations = dilations\n",
    "        self.out_channel = out_channel\n",
    "        self.shape_before_bottleneck = shape_before_bottleneck\n",
    "\n",
    "        self.flatten_shape = torch.numel(torch.ones(self.shape_before_bottleneck))\n",
    "        self.fc = nn.Linear(latent_space_dim, self.flatten_shape)\n",
    "\n",
    "        # dim assertion \n",
    "        assert len(self.conv_kernels) == len(self.conv_strides) == len(self.paddings) == len(self.output_paddings) == len(self.dilations)\n",
    "\n",
    "        self.convtransposes = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "            (\n",
    "            f\"Convolution_Transpose_Block{i+1}\",          \n",
    "            ConvTranspose2dBlock(conv_filters_in=self.conv_filters[i],\n",
    "                                conv_filters_out=self.conv_filters[i+1],\n",
    "                                conv_kernels=self.conv_kernels[i],\n",
    "                                conv_strides=self.conv_strides[i],\n",
    "                                paddings=self.paddings[i],\n",
    "                                output_paddings=self.output_paddings[i],\n",
    "                                dilations=self.dilations[i])\n",
    "            )\n",
    "            \n",
    "            for i in range(len(self.conv_filters) - 1)\n",
    "\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.output_convolution = nn.ConvTranspose2d(in_channels=self.conv_filters[-1],\n",
    "                                                    out_channels=3, # colored images\n",
    "                                                    kernel_size=self.conv_kernels[0],\n",
    "                                                    stride=(1, 1),\n",
    "                                                    padding=(1, 1),\n",
    "                                                    output_padding=(0, 0),\n",
    "                                                    dilation=(2, 2)\n",
    "\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = x.view(self.shape_before_bottleneck)\n",
    "\n",
    "        for convtransposeblock in self.convtransposes:\n",
    "\n",
    "            x = convtransposeblock(x)\n",
    "\n",
    "        x = self.output_convolution(x)\n",
    "\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_shape : List[int],\n",
    "                conv_filters : List[Tuple[int]],\n",
    "                conv_kernels : List[Tuple[int]],\n",
    "                conv_strides : List[Tuple[int]],\n",
    "                paddings : List[Tuple[int]],\n",
    "                output_paddings : List[Tuple[int]],\n",
    "                dilations: List[Tuple[int]],\n",
    "                latent_space_dim : int,\n",
    "                **kwargs):\n",
    "\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        \n",
    "        self.encoder = Encoder(input_shape=input_shape,\n",
    "                                conv_filters=conv_filters,\n",
    "                                conv_kernels=conv_kernels,\n",
    "                                conv_strides=conv_strides,\n",
    "                                paddings=paddings,\n",
    "                                dilations=dilations,\n",
    "                                latent_space_dim=latent_space_dim\n",
    "                                )\n",
    "\n",
    "        self.shape_before_bottleneck = self.encoder.shape_before_bottleneck\n",
    "\n",
    "        self.decoder = Decoder(latent_space_dim=latent_space_dim,\n",
    "                                shape_before_bottleneck=self.encoder.shape_before_bottleneck,\n",
    "                                conv_filters=conv_filters[::-1],\n",
    "                                conv_kernels=conv_kernels[::-1],\n",
    "                                conv_strides=conv_strides[::-1],\n",
    "                                paddings=paddings[::-1],\n",
    "                                output_paddings=output_paddings,\n",
    "                                dilations=dilations[::-1],\n",
    "                                out_channel=3\n",
    "                                )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, (mu, log_sigma) = self.encoder(x)\n",
    "            \n",
    "        x_prime = self.decoder(z)\n",
    "\n",
    "        return z, mu, log_sigma, x_prime\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "\n",
    "        if eps is None:\n",
    "            eps = torch.randn([1, self.latent_space_dim])\n",
    "            return self.decoder(eps)\n",
    "\n",
    "        else:\n",
    "            return self.decoder(eps)\n",
    "\n",
    "    def reconstruct(self, images):\n",
    "        latent_representations = self.encoder(images)\n",
    "        reconstructed_images = self.decoder(latent_representations)\n",
    "\n",
    "        return reconstructed_images, latent_representations\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_div(mu, log_sigma):\n",
    "        loss = -0.5 * torch.sum(1 + log_sigma + torch.square(mu) -torch.exp(log_sigma), 1)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def loss_fn(self, x, x_prime, mu, log_sigma):\n",
    "\n",
    "        kld_loss = self.kl_div\n",
    "        recon_loss = nn.MSELoss()\n",
    "\n",
    "        kld = kld_loss(mu, log_sigma)\n",
    "        recon = recon_loss(x, x_prime)\n",
    "\n",
    "        loss = kld + recon\n",
    "\n",
    "        return loss, kld, recon\n",
    "\n",
    "\n",
    "    \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =\"\"\"\n",
    "VAE(input_shape=[3, 64, 64],\n",
    "    conv_filters=[3, 16, 32, 64 , 128],\n",
    "    conv_kernels=[(5, 5), (3, 3), (3, 3), (3, 3)],\n",
    "    conv_strides=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    paddings=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    output_paddings=[(0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "    dilations=[1, 1, 1, 1],\n",
    "    latent_space_dim=1024)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(input_shape=[3, 64, 64],\n",
    "    conv_filters=[3, 32, 64, 128 , 256],\n",
    "    conv_kernels=[(5, 5), (3, 3), (3, 3), (3, 3)],\n",
    "    conv_strides=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    paddings=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    output_paddings=[(0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "    dilations=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    latent_space_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        \n",
    "        path = \"alltanks.npy\"\n",
    "\n",
    "        images_data = np.load(path)\n",
    "\n",
    "        data = np.swapaxes(images_data, 3, 1)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            return self.transform(self.data[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    # Convert ndarrays to Tensors\n",
    "    def __call__(self, sample):\n",
    "        x = sample\n",
    "        return torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TanksDataset(transform=ToTensor()), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.9843,  0.9843,  0.9843,  ...,  0.2235,  0.2706,  0.2000],\n",
      "          [ 0.9922,  0.9922,  0.9843,  ...,  0.1843,  0.2392,  0.2078],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.2157,  0.2549,  0.1765],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  0.9922,  ...,  0.2000, -0.0431,  0.0510],\n",
      "          [ 1.0000,  0.9922,  0.9843,  ...,  0.1059, -0.0824,  0.0275],\n",
      "          [ 1.0000,  0.9922,  0.9843,  ...,  0.0118, -0.0588,  0.1216]],\n",
      "\n",
      "         [[ 0.9843,  0.9843,  0.9843,  ...,  0.1294,  0.1765,  0.1922],\n",
      "          [ 0.9922,  0.9922,  0.9843,  ...,  0.1608,  0.2078,  0.2314],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.1373,  0.2000,  0.2471],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  0.9922,  ...,  0.0275, -0.0980,  0.0510],\n",
      "          [ 1.0000,  0.9922,  0.9843,  ..., -0.0745, -0.1451,  0.0510],\n",
      "          [ 1.0000,  0.9922,  0.9843,  ..., -0.1686, -0.1294,  0.1294]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  ..., -0.2863, -0.2784, -0.4039],\n",
      "          [ 0.9922,  1.0000,  1.0000,  ..., -0.2863, -0.3098, -0.3804],\n",
      "          [ 0.9922,  0.9922,  1.0000,  ..., -0.3020, -0.2863, -0.4275],\n",
      "          ...,\n",
      "          [ 1.0000,  1.0000,  0.9922,  ..., -0.1922, -0.4196, -0.4118],\n",
      "          [ 1.0000,  0.9922,  1.0000,  ..., -0.2627, -0.4588, -0.4745],\n",
      "          [ 0.9922,  0.9922,  1.0000,  ..., -0.3490, -0.4353, -0.3569]]]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "iterdata = iter(train_loader)\n",
    "print(iterdata.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae, dataloader, epochs=1, device=torch.device(\"cpu\")):\n",
    "        vae = vae.to(device)\n",
    "        vae = vae.double()\n",
    "        #transform = T.ConvertImageDtype(dtype=torch.double)\n",
    "        optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "        reported_loss = []\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            collective_loss = []\n",
    "            for _, x in tqdm(enumerate(dataloader)):\n",
    "\n",
    "                x.to(device)\n",
    "                \n",
    "                #x = transform(images)\n",
    "\n",
    "                #assert x.dtype == torch.double\n",
    "\n",
    "                _, mu, log_sigma, x_prime = vae.forward(x.double())\n",
    "\n",
    "                loss, recon, kld = vae.loss_fn(x, x_prime, mu, log_sigma)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                collective_loss.append([recon.item(), kld.item()])\n",
    "            \n",
    "            np_collective_loss = np.array(collective_loss)\n",
    "            \n",
    "            average_loss = np.mean(np_collective_loss, axis=1)\n",
    "\n",
    "            reported_loss.append(average_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} finished!\", f\"reconstruction_loss = {average_loss[0]} || KL-Divergence = {average_loss[1]}\", sep=\"\\n\")\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    to_img = T.ToPILImage()\n",
    "                    \n",
    "                    example = vae.sample()\n",
    "                    \n",
    "                    img_example = to_img(example)\n",
    "\n",
    "                    img_example.save(f\"result_at_epoch_{epoch+1}.png\")\n",
    "                    \n",
    "        \n",
    "        print(\"Training Finished!\")\n",
    "\n",
    "        return np.array(list(zip(range(epochs), average_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.52 GiB (GPU 0; 8.00 GiB total capacity; 7.03 GiB already allocated; 0 bytes free; 7.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(vae, train_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(vae, dataloader, epochs, device)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(vae, dataloader, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m----> 2\u001b[0m         vae \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      3\u001b[0m         vae \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mdouble()\n\u001b[0;32m      4\u001b[0m         \u001b[39m#transform = T.ConvertImageDtype(dtype=torch.double)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=894'>895</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=895'>896</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=896'>897</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=898'>899</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=568'>569</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=569'>570</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=571'>572</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=572'>573</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=573'>574</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=574'>575</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=568'>569</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=569'>570</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=571'>572</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=572'>573</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=573'>574</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=574'>575</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=589'>590</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=590'>591</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=591'>592</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=592'>593</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=593'>594</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=594'>595</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=893'>894</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=894'>895</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=895'>896</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/torch/nn/modules/module.py?line=896'>897</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.52 GiB (GPU 0; 8.00 GiB total capacity; 7.03 GiB already allocated; 0 bytes free; 7.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train(vae, train_loader, epochs=100, device=torch.device(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51f49de4ecea0dc171f2857edc25e2cf46e665b2859ff26ff78277ff2a4d1a07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
