{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first part of this notebook will be dedicated to the basic VAE architecture\n",
    "\n",
    "# The second part will be dedicated to the VAE with skip-connections\n",
    "\n",
    "# both model will be tested by passing [64, 3, 64, 64] torch.randn tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                conv_filters_in: int,\n",
    "                conv_filters_out: int,\n",
    "                conv_kernels: Tuple[int],\n",
    "                conv_strides: Tuple[int],\n",
    "                paddings: Tuple[int],\n",
    "                dilations: Tuple[int],\n",
    "                **kwargs):\n",
    "        \n",
    "        super(Conv2dBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_filters_in = conv_filters_in\n",
    "        self.conv_filters_out = conv_filters_out\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.dilations = dilations\n",
    "\n",
    "\n",
    "        self.conv2d = nn.Conv2d(in_channels=self.conv_filters_in, \n",
    "                                out_channels=self.conv_filters_out, \n",
    "                                kernel_size=self.conv_kernels,\n",
    "                                stride=self.conv_strides,\n",
    "                                padding=self.paddings,\n",
    "                                dilation=self.dilations)\n",
    "\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=self.conv_kernels, \n",
    "                                    stride=self.conv_strides,\n",
    "                                    padding=self.paddings,\n",
    "                                    dilation=self.dilations)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=self.conv_filters_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        x = self.maxpool2d(x)\n",
    "\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTranspose2dBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                conv_filters_in: int,\n",
    "                conv_filters_out: int,\n",
    "                conv_kernels: Tuple[int],\n",
    "                conv_strides: Tuple[int],\n",
    "                paddings: Tuple[int],\n",
    "                output_paddings: Tuple[int],\n",
    "                dilations: Tuple[int],\n",
    "                **kwargs):\n",
    "\n",
    "        super(ConvTranspose2dBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_filters_in = conv_filters_in\n",
    "        self.conv_filters_out = conv_filters_out\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.output_paddings = output_paddings\n",
    "        self.dilations = dilations\n",
    "\n",
    "        self.convtranspose2d = nn.ConvTranspose2d(in_channels=self.conv_filters_in, \n",
    "                                                out_channels=self.conv_filters_out, \n",
    "                                                kernel_size=self.conv_kernels,\n",
    "                                                stride=self.conv_strides,\n",
    "                                                padding=self.paddings,\n",
    "                                                output_padding=self.output_paddings,\n",
    "                                                dilation=self.dilations)\n",
    "\n",
    "        self.upsampling = nn.UpsamplingBilinear2d(scale_factor=self.conv_strides[0])\n",
    "\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=self.conv_filters_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.convtranspose2d(x)\n",
    "\n",
    "        x = self.upsampling(x)\n",
    "\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_shape: List[int],\n",
    "                conv_filters:List[int], # must have 1 more element than others -> FIRST element must be 3 for colored images\n",
    "                conv_kernels: List[Tuple[int]],\n",
    "                conv_strides: List[Tuple[int]],\n",
    "                paddings: List[Tuple[int]],\n",
    "                dilations: List[Tuple[int]],\n",
    "                latent_space_dim: int,\n",
    "                **kwargs):\n",
    "    \n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        \n",
    "        self.conv_filters = conv_filters # [2, 4, 8]\n",
    "        self.conv_kernels = conv_kernels \n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.dilations = dilations\n",
    "        self.latent_space_dim = latent_space_dim\n",
    "\n",
    "        \n",
    "        # dim assertion\n",
    "\n",
    "        assert len(self.conv_kernels) == len(self.conv_strides) == len(self.paddings)\n",
    "\n",
    "        self.convblocks = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "            (\n",
    "            f\"Convolution_Block_{i+1}\",\n",
    "            Conv2dBlock(conv_filters_in=self.conv_filters[i],\n",
    "                        conv_filters_out=self.conv_filters[i+1],\n",
    "                        conv_kernels=self.conv_kernels[i],\n",
    "                        conv_strides=self.conv_strides[i],\n",
    "                        paddings=self.paddings[i],\n",
    "                        dilations=self.dilations[i]).float()\n",
    "            )\n",
    "            \n",
    "            for i in range(len(self.conv_filters) - 1)\n",
    "            \n",
    "                ]\n",
    "            )\n",
    "        )   \n",
    "\n",
    "        self.shape_before_bottleneck = self._calculate_shape_before_bottleneck(input_shape).shape\n",
    "\n",
    "        self.flatten_shape = torch.numel(self._calculate_shape_before_bottleneck(input_shape))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.mu = nn.Linear(self.flatten_shape, self.latent_space_dim)\n",
    "\n",
    "        self.log_sigma = nn.Linear(self.flatten_shape, self.latent_space_dim)\n",
    "\n",
    "        \n",
    "\n",
    "    def _calculate_shape_before_bottleneck(self, input_shape: List[int]):\n",
    "\n",
    "        x = torch.ones(input_shape, dtype=torch.float32).to(device)\n",
    "\n",
    "        x = torch.unsqueeze(x, 0) \n",
    "\n",
    "        for convblock in self.convblocks:\n",
    "\n",
    "            x = convblock(x)\n",
    "\n",
    "        return x   \n",
    "\n",
    "    def _reparameterized(self, mu, log_sigma):\n",
    "        \n",
    "        eps = torch.randn(size=mu.shape, dtype=torch.float32).to(device)\n",
    "        \n",
    "        sample_point = mu + torch.exp(log_sigma / 2) * eps\n",
    "        \n",
    "        return sample_point    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for convblock in self.convblocks:\n",
    "            \n",
    "            x = convblock(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_sigma= self.log_sigma(x)\n",
    "\n",
    "        x = self._reparameterized(mu, log_sigma)\n",
    "\n",
    "        return x, (mu, log_sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                latent_space_dim :int,\n",
    "                shape_before_bottleneck : torch.Size,\n",
    "                conv_filters : List[int], # must have 1 more element than others -> LAST element must be 3 for colored images\n",
    "                conv_kernels : List[Tuple[int]],\n",
    "                conv_strides : List[Tuple[int]],\n",
    "                paddings : List[Tuple[int]],\n",
    "                output_paddings : List[Tuple[int]],\n",
    "                dilations : List[Tuple[int]],\n",
    "                out_channel : int,\n",
    "                **kwargs):\n",
    "\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_filters = conv_filters\n",
    "        self.conv_kernels = conv_kernels\n",
    "        self.conv_strides = conv_strides\n",
    "        self.paddings = paddings\n",
    "        self.output_paddings = output_paddings\n",
    "        self.dilations = dilations\n",
    "        self.out_channel = out_channel\n",
    "        self.shape_before_bottleneck = shape_before_bottleneck\n",
    "\n",
    "        self.flatten_shape = torch.numel(torch.ones(self.shape_before_bottleneck, dtype=torch.float32, device=device))\n",
    "        self.fc = nn.Linear(latent_space_dim, self.flatten_shape)\n",
    "\n",
    "        # dim assertion \n",
    "        assert len(self.conv_kernels) == len(self.conv_strides) == len(self.paddings) == len(self.output_paddings) == len(self.dilations)\n",
    "\n",
    "        self.convtransposes = nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "            (\n",
    "            f\"Convolution_Transpose_Block{i+1}\",          \n",
    "            ConvTranspose2dBlock(conv_filters_in=self.conv_filters[i],\n",
    "                                conv_filters_out=self.conv_filters[i+1],\n",
    "                                conv_kernels=self.conv_kernels[i],\n",
    "                                conv_strides=self.conv_strides[i],\n",
    "                                paddings=self.paddings[i],\n",
    "                                output_paddings=self.output_paddings[i],\n",
    "                                dilations=self.dilations[i])\n",
    "            )\n",
    "            \n",
    "            for i in range(len(self.conv_filters) - 1)\n",
    "\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.output_convolution = nn.ConvTranspose2d(in_channels=self.conv_filters[-1],\n",
    "                                                    out_channels=3, # colored images\n",
    "                                                    kernel_size=self.conv_kernels[0],\n",
    "                                                    stride=(1, 1),\n",
    "                                                    padding=(1, 1),\n",
    "                                                    output_padding=(0, 0),\n",
    "                                                    dilation=(2, 2)\n",
    "\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = x.view(self.shape_before_bottleneck)\n",
    "\n",
    "        for convtransposeblock in self.convtransposes:\n",
    "\n",
    "            x = convtransposeblock(x)\n",
    "\n",
    "        x = self.output_convolution(x)\n",
    "\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_shape : List[int],\n",
    "                conv_filters : List[Tuple[int]],\n",
    "                conv_kernels : List[Tuple[int]],\n",
    "                conv_strides : List[Tuple[int]],\n",
    "                paddings : List[Tuple[int]],\n",
    "                output_paddings : List[Tuple[int]],\n",
    "                dilations: List[Tuple[int]],\n",
    "                latent_space_dim : int,\n",
    "                **kwargs):\n",
    "\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.latent_space_dim = latent_space_dim\n",
    "        \n",
    "        self.encoder = Encoder(input_shape=input_shape,\n",
    "                                conv_filters=conv_filters,\n",
    "                                conv_kernels=conv_kernels,\n",
    "                                conv_strides=conv_strides,\n",
    "                                paddings=paddings,\n",
    "                                dilations=dilations,\n",
    "                                latent_space_dim=latent_space_dim\n",
    "                                )\n",
    "\n",
    "        self.shape_before_bottleneck = self.encoder.shape_before_bottleneck\n",
    "\n",
    "        self.decoder = Decoder(latent_space_dim=latent_space_dim,\n",
    "                                shape_before_bottleneck=self.encoder.shape_before_bottleneck,\n",
    "                                conv_filters=conv_filters[::-1],\n",
    "                                conv_kernels=conv_kernels[::-1],\n",
    "                                conv_strides=conv_strides[::-1],\n",
    "                                paddings=paddings[::-1],\n",
    "                                output_paddings=output_paddings,\n",
    "                                dilations=dilations[::-1],\n",
    "                                out_channel=3\n",
    "                                )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, (mu, log_sigma) = self.encoder(x)\n",
    "            \n",
    "        x_prime = self.decoder(z)\n",
    "\n",
    "        return z, mu, log_sigma, x_prime\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "\n",
    "        if eps is None:\n",
    "            eps = torch.randn([1, self.latent_space_dim])\n",
    "            return self.decoder(eps)\n",
    "\n",
    "        else:\n",
    "            return self.decoder(eps)\n",
    "\n",
    "    def reconstruct(self, images):\n",
    "        latent_representations = self.encoder(images)\n",
    "        reconstructed_images = self.decoder(latent_representations)\n",
    "\n",
    "        return reconstructed_images, latent_representations\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_div(mu, log_sigma):\n",
    "        loss = -0.5 * torch.sum(1 + log_sigma + torch.square(mu) -torch.exp(log_sigma), 1)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def loss_fn(self, x, x_prime, mu, log_sigma):\n",
    "\n",
    "        kld_loss = self.kl_div\n",
    "        recon_loss = nn.MSELoss()\n",
    "\n",
    "        kld = kld_loss(mu, log_sigma)\n",
    "        recon = recon_loss(x, x_prime)\n",
    "\n",
    "        loss = kld + recon\n",
    "\n",
    "        return loss, kld, recon\n",
    "\n",
    "\n",
    "    \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =\"\"\"\n",
    "VAE(input_shape=[3, 64, 64],\n",
    "    conv_filters=[3, 16, 32, 64 , 128],\n",
    "    conv_kernels=[(5, 5), (3, 3), (3, 3), (3, 3)],\n",
    "    conv_strides=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    paddings=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    output_paddings=[(0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "    dilations=[1, 1, 1, 1],\n",
    "    latent_space_dim=1024)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(input_shape=[3, 64, 64],\n",
    "    conv_filters=[3, 32, 64, 128 , 256],\n",
    "    conv_kernels=[(5, 5), (3, 3), (3, 3), (3, 3)],\n",
    "    conv_strides=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    paddings=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    output_paddings=[(0, 0), (0, 0), (0, 0), (0, 0)],\n",
    "    dilations=[(1, 1), (1, 1), (1, 1), (1, 1)],\n",
    "    latent_space_dim=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanksDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        \n",
    "        path = \"alltanks.npy\"\n",
    "\n",
    "        images_data = np.load(path)\n",
    "\n",
    "        data = np.swapaxes(images_data, 3, 1)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            return self.transform(self.data[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    # Convert ndarrays to Tensors\n",
    "    def __call__(self, sample):\n",
    "        x = sample\n",
    "        return torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TanksDataset(transform=ToTensor()), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata = iter(train_loader)\n",
    "print(iterdata.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae, dataloader, epochs=1, device=torch.device(\"cpu\")):\n",
    "        vae = vae.to(device)\n",
    "        vae = vae.double()\n",
    "        #transform = T.ConvertImageDtype(dtype=torch.double)\n",
    "        optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "        reported_loss = []\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            collective_loss = []\n",
    "            for _, x in tqdm(enumerate(dataloader)):\n",
    "\n",
    "                x.to(device)\n",
    "                \n",
    "                #x = transform(images)\n",
    "\n",
    "                #assert x.dtype == torch.double\n",
    "\n",
    "                _, mu, log_sigma, x_prime = vae.forward(x.double())\n",
    "\n",
    "                loss, recon, kld = vae.loss_fn(x, x_prime, mu, log_sigma)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                collective_loss.append([recon.item(), kld.item()])\n",
    "            \n",
    "            np_collective_loss = np.array(collective_loss)\n",
    "            \n",
    "            average_loss = np.mean(np_collective_loss, axis=1)\n",
    "\n",
    "            reported_loss.append(average_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} finished!\", f\"reconstruction_loss = {average_loss[0]} || KL-Divergence = {average_loss[1]}\", sep=\"\\n\")\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    to_img = T.ToPILImage()\n",
    "                    \n",
    "                    example = vae.sample()\n",
    "                    \n",
    "                    img_example = to_img(example)\n",
    "\n",
    "                    img_example.save(f\"result_at_epoch_{epoch+1}.png\")\n",
    "                    \n",
    "        \n",
    "        print(\"Training Finished!\")\n",
    "\n",
    "        return np.array(list(zip(range(epochs), average_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(vae, train_loader, epochs=100, device=torch.device(\"cuda\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51f49de4ecea0dc171f2857edc25e2cf46e665b2859ff26ff78277ff2a4d1a07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
